{
    /*
        Increment the Id for every new desired state, otherwise it will be ignored.
        This also works as a reference to "RunningSince" fields - these can never be
        higher than the Id.
    */
    "Id": 1,
    "RonDBRunning": true,
    // A logical clock to perform rolling restarts of RonDB; it can never be higher than the Id
    "RonDBRunningSince": 1,
    // Make sure to adjust RonDBDownloadLink when changing this
    "RonDBVersion": "22.10.1",
    /*
        The url nginx/hopsworks_proxy is a reverse proxy to repo.hops.works, 
        which is where Hopsworks hosts RonDB builds:
    */
    "RonDBDownloadLink": "http://nginx/hopsworks_proxy/master/rondb-22.10.1-linux-glibc2.35-arm64_v8.tar.gz",
    // A logical clock to perform rolling restarts of the ndb-agent; it can never be higher than the Id
    "NdbAgentRunningSince": 1,
    /*
        Version 0.999 is an imaginary version which is identical to the current
        build and is stored on the nginx server. It can be used to test rolling
        software upgrades of the ndb-agent. The other available version is 1.0.

        Make sure to also adjust NdbAgentDownloadLink when changing NdbAgentVersion.
    */
    "NdbAgentVersion": "0.999",
    "NdbAgentDownloadLink": "http://nginx/local/ndb-agent-0.999.tgz",
    "SingleBackups": [
        /*
            This is an example of a backup that is saved on a Docker volume.
            As this is the first desired state, this backup will be run right
            away and therefore almost be empty.
            If it is removed in a following desired state, the backup will be
            deleted. 
            The same backup can be referenced in the field "RestoreFromBackup".
        */
        {
            "Id": 123,
            "SinkType": "docker",
            "SinkParameters": {
                "TestParameter": "test-value"
            }
        }
    ],
    /*
        This backup has to exist before it can be set here. This field can only be
        set if RonDBRunning has never been set to true before. Therefore, a cluster
        can never restore its own backup.

        Make sure that this backup is also referenced in SingleBackups.

        "RestoreFromBackup": {
            "Id": 123,
            "SinkType": "docker",
            "SinkParameters": {
                "TestParameter": "test-value"
            }
        },
    */
    /*
        RonDB-native configuration parameter to avoid data nodes from allocating all
        the memory in their containers. Some memory will also be needed for the ndb-agent.
    */
    "TotalMemoryConfig": "2400M",
    /*
        These are the parameters that determine how many data nodes we will have running.
        Number of running data node container = ActiveReplicationFactor * NumNodeGroups
    */
    "ActiveReplicationFactor": 1, // Can be changed to add replicas to every node group
    "MaxReplicationFactor": 3, // Immutable
    "NumNodeGroups": 1, // Immutable
    /*
        Make sure that these nodes exist as containers before specifying them in the desired state.
        The nodes specified here are listed in the docker-compose file.

        However, in case the "Ip" field is empty, no container is required and they will 
        correspond to slots without Hostnames in the config.ini. This only works for mysqld
        and ndb_api slots though.

        Possible RonDBTypes: ndb_mgm, ndb_ndbd, mysqld, ndb_api
    */
    "ExternallyManagedNodes": [
        // This bootstrap container can currently not be removed
        {
            "RonDBType": "ndb_mgm",
            "Ip": "bootstrap_mgm",
            "Port": 1186
        }
    ],
    /*
        Immutable fields. They are RonDB-native parameters that create slots
        in the config.ini. We set a number to allow scaling up/down when the
        cluster is running. If they are not used, they are deactivated.
    */
    "NumTotalMysqldSlots": 10,
    "NumTotalApiSlots": 16,
    /*
        Immutable fields. Whenever new MySQLd/Api Vms are requested, these number
        of slots will be allocated to them.
    */
    "NumMysqldSlotsPerVm": 4,
    "NumApiSlotsPerVm": 4,
    /*
        Sometimes, new desired states cannot be reached without allowing
        temporary VMs. E.g.: upgrading the RonDB version with a replication
        factor of 1.
    */
    "AllowRequestingTemporaryVms": true,
    /*
        The flask server will claim that the new containers it spawns
        are in the given Region / Availability Zone. The ndb-agent
        will distribute each service evenly across availability zone.
        E.g.: one replica per AZ
    */
    "Region": "europe-1",
    "AvailabilityZones": [
        "soedermalm"
    ],
    /*
        The flask server supports the VmType:
            mini, medium, large, xlarge, xxlarge
        These have differences in container memory limits.

        Only increase the VmType; reducing beyond the defaults may crash
        the programs running on the corresponding container.
    */
    "VmTypeMgmds": "mini",
    "VmTypeApis": "medium",
    "VmTypeMysqlds": "large",
    "VmTypeNdbds": "xlarge",
    // Use this to scale MySQLd servers & Api containers:
    "NumManagedMysqldVms": 1,
    "NumManagedApiVms": 1,
    /*
        The flask server does not have variable storage size containers implemented
        and so it will simply claim that the spawned containers have a given storage
        size. Also, the storage is always attached to containers and there is no logic
        implemented to support shared network disks / volumes. Therefore, when
        changing these fields for a live cluster, the containers will be replaced.
    */
    "StorageGiBPerMgmdVm": 4,
    "StorageGiBPerNdbdVm": 30,
    "StorageGiBPerMysqldVm": 15,
    "StorageGiBPerApiVm": 10,
    /*
        These are immutable fields that will be fixed in RonDB's config.ini.
        Take these into account when adding ExternallyManagedNodes on a running
        cluster.
    */
    "DefaultMgmdPort": 1186,
    "DefaultNdbdPort": 11860,
    "DefaultMysqldPort": 3306
}